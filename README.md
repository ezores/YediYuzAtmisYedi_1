```md
# RÃ©seau de Neurones pour la Reconnaissance de la Parole
```
Ce projet implÃ©mente un rÃ©seau de neurones Ã  rÃ©tropropagation du gradient dâ€™erreur pour l'extraction de caractÃ©ristiques et la reconnaissance de la parole. L'implÃ©mentation est basÃ©e sur NumPy pour des performances optimisÃ©es.
```
ğŸ“‚ Structure du Projet


/Projet_NN
â”‚â”€â”€ README.md            # Documentation du projet
â”‚â”€â”€ main.py              # Script principal d'entraÃ®nement et d'Ã©valuation
â”‚â”€â”€ mlp.py               # ImplÃ©mentation du rÃ©seau de neurones multicouches
â”‚â”€â”€ mlp_math.py          # Fonctions mathÃ©matiques (activations, dÃ©rivÃ©es, etc.)
â”‚â”€â”€ propagation.py       # Propagation avant et arriÃ¨re du rÃ©seau
â”‚â”€â”€ training.py          # Gestion de l'entraÃ®nement et de la validation
â”‚â”€â”€ file_utils.py        # Fonctions de gestion des fichiers et logs
â”‚â”€â”€ OutputFiles/         # Dossier contenant les fichiers de donnÃ©es
â”‚â”€â”€ pictures/            # Visualisations du rÃ©seau
â”‚â”€â”€ requirements.txt     # DÃ©pendances Python

```
ğŸš€ Installation

1ï¸âƒ£ PrÃ©requis
- Python 3.8+
- pip installÃ©
``` ```
2ï¸âƒ£ Cloner le dÃ©pÃ´t
```sh
git clone [https://github.com/utilisateur/Projet_NN.git](https://github.com/ezores/YediYuzAtmisYedi_1.git)
cd YediYuzAtmisYedi
```

3ï¸âƒ£ Installer les dÃ©pendances
```sh
pip install -r requirements.txt
```
ğŸ¯ Utilisation
Pour dÃ©marer le programme, Ã©crivez cette commande dans le terminal:
    -python main.py

Suivit de :
help :pour voir un exemple des possibilitÃ©es dans le terminal. LE terminal devrait montrer la ligne suivante:
usage: main.py [-h] (--train | --vc | --test) [--eta ETA] [--neurons NEURONS [NEURONS ...]] [--activations ACTIVATIONS [ACTIVATIONS ...]] [--base {40,50,60}] [--epochs EPOCHS] [--adaptive] [--noise NOISE]
main.py: error: one of the arguments --train --vc --test is required

le premier choix est obligatoire et doit Ãªtre uniquement l'un des trois choix entre:
--train: pour faire l'entraÃ®nement du MLP.
--vc:    pour faire la valiudation croisÃ© du MLP.
--test:  pour tester le MLP.

Si aucun autre paramÃ¨tre n'est entrÃ©e, des paramÃ¨tres seront demandÃ© Ã  l'utilisateur pour chacune des possibilitÃ©es. Pour chaque paramÃ¨tre deamndÃ©, il est possible d'entrer une valeur dÃ©sirÃ©e
ou de ne rien entrer pour garder la valeur proposÃ© par le terminal.

EntraÃ®ner un modÃ¨le
L'entraÃ®nement d'un modÃ¨le peut Ãªtre lancÃ© avec diffÃ©rents hyperparamÃ¨tres :
```sh
python main.py --train --eta 0.001 --neurons 128 64 --activations relu --base 60 --epochs 100 --adaptive --noise 0.1
```
ğŸ”¹ Exemples de paramÃ¨tres :
| Argument | Description |
|----------|------------|
| `--eta` | Taux dâ€™apprentissage |
| `--neurons` | Nombre de neurones par couche cachÃ©e.Exemple: pour 1 couche de 10 neurones et une 2e couche de 5 neurones, ont met (--neurons 10 5) |
| `--activations` | Fonction d'activation (`relu`, `sigmoide`, `tanh`, etc.) |
| `--base` | Base de donnÃ©es utilisÃ©e (40, 50 ou 60) |
| `--epochs` | Nombre d'Ã©poques |
| `--adaptive` | Active l'apprentissage adaptatif |
| `--noise` | Ajoute un bruit alÃ©atoire aux donnÃ©es

``` ```
ğŸ› ï¸ Personnalisation

Modifier les fonctions d'activation
Les fonctions d'activation se trouvent dans mlp_math.py. 
Exemple : Ajouter une nouvelle activation Leaky ReLU.
```python
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

def leaky_relu_derivative(a, alpha=0.01):
    return np.where(a > 0, 1, alpha)

activation_functions['leaky_relu'] = (leaky_relu, leaky_relu_derivative)
```
Ensuite, utiliser :
```sh
python main.py --train --activations leaky_relu
```

ğŸ“Š Visualisation

Les poids et lâ€™architecture du rÃ©seau peuvent Ãªtre visualisÃ©s et gÃ©nÃ©rÃ©s automatiquement aprÃ¨s le fonctionnement du code et l'ancien s'efface quand on relance le code.

ğŸ“ Les images gÃ©nÃ©rÃ©es sont stockÃ©es dans le dossier `pictures/`. et HTML comme le fichier : `Visualisation_MLP.html` 

ğŸï¸ Optimisations & Performances

ğŸ”¹ Passage de SymPy Ã  NumPy â†’ AmÃ©lioration significative du temps d'exÃ©cution.  
ğŸ”¹ Propagation vectorisÃ©e avec `np.dot()` â†’ AccÃ©lÃ©ration des calculs.  
ğŸ”¹ Application de dropout (uniquement en entraÃ®nement) pour limiter le surapprentissage.  
ğŸ”¹ Normalisation des entrÃ©es â†’ Meilleure stabilitÃ© du modÃ¨le.  

## ğŸ“œ Licence

Ce projet est sous licence **MIT**.  
Libre d'utilisation, de modification et de distribution.

ğŸš€ PrÃªt Ã  entraÃ®ner un rÃ©seau de neurones performant pour la reconnaissance de la parole !

